{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help function\n",
    "from transfer_learning import NeuralNet\n",
    "from dataset_loader import data_loader, all_filter, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# file name and data path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = os.getcwd()\n",
    "file_name = 'data/CrystGrowthDesign_SI.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13506, 5)\n",
      "(13506,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = ['void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest']\n",
    "one_filter_columns = ['H2@100 bar/243K (wt%)'] \n",
    "another_filter_columns = ['H2@100 bar/130K (wt%)'] \n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "\n",
    "# extract descriptors and gas adsorptions\n",
    "one_property = one_filter(data, one_filter_columns)\n",
    "descriptors = get_descriptors(data, descriptor_columns)\n",
    "\n",
    "# prepare training inputs and outputs\n",
    "X = np.array(descriptors.values, dtype=np.float32)\n",
    "y = np.array(one_property.values, dtype=np.float32).reshape(len(X), )\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X = data_scaler(X)\n",
    "y = data_scaler(y.reshape(-1, 1)).reshape(len(X),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source task training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 374/10000 [00:16<06:55, 23.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3f7b2c146edc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## hyper-parameters\n",
    "input_size = 5\n",
    "hidden_size_1 = 128\n",
    "hidden_size_2 = 64\n",
    "output_size = 1\n",
    "learning_rate = 0.00002\n",
    "\n",
    "## model, loss, and optimizer\n",
    "model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## train, val, test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "# Training\n",
    "scores_epochs = list()\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    inputs = torch.from_numpy(X_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "\n",
    "    outputs = model(inputs).view(-1,)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        inputs_val = torch.from_numpy(X_val)\n",
    "        labels_val = torch.from_numpy(y_val)\n",
    "\n",
    "        outputs_val = model(inputs_val).view(-1,)\n",
    "        score = r2_score(labels_val.data.numpy(), outputs_val.data.numpy())\n",
    "#         print('Predictive accuracy on validation set at epoch {}/{} is {}'.format(epoch, num_epochs, score)) \n",
    "        scores_epochs.append(score)\n",
    "#     if len(scores_epochs) >= 2:\n",
    "#             if score < scores_epochs[-2]:\n",
    "#                 break\n",
    "\n",
    "# torch.save(model.state_dict(), 'model_H2.ckpt')  \n",
    "plt.plot(np.arange(0, num_epochs, 5), scores_epochs, color='red')\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Pre Acc on Val set', size=15)\n",
    "plt.savefig('Manuscript/H2_val.png', bbox_inches='tight', dpi=500)\n",
    "print('The predictive accuracy on test set is {}'.format(\n",
    "                r2_score(torch.from_numpy(y_test).data.numpy(), model(torch.from_numpy(X_test)).view(-1,).data.numpy())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct learning and transfer learning on target tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning(transfer=False, nsamples=None, nbatches=None,name):\n",
    "    seeds = np.arange(nbatches)\n",
    "    Ns = list()\n",
    "    scores_epochs = list()\n",
    "    scores_test = list()\n",
    "    scores_train = list()\n",
    "\n",
    "    pred_tests = list()\n",
    "    grt_train_X = list()\n",
    "    grt_test_X = list()\n",
    "    grt_tests = list()\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        data_small = data.sample(n=nsamples, random_state=seed)\n",
    "\n",
    "        another_property = one_filter(data_small, another_filter_columns)\n",
    "        descriptors_small = get_descriptors(data_small, descriptor_columns)\n",
    "\n",
    "        X_small = np.array(descriptors_small.values, dtype=np.float32)\n",
    "        y_small = np.array(another_property.values, dtype=np.float32).reshape(len(X_small), )\n",
    "        \n",
    "        X_small = data_scaler(X_small)\n",
    "        y_small = data_scaler(y_small.reshape(-1, 1)).reshape(len(X_small),)\n",
    "\n",
    "        ## hyper-parameters\n",
    "        input_size = 5\n",
    "        hidden_size_1 = 128\n",
    "        hidden_size_2 = 64\n",
    "        output_size = 1\n",
    "        learning_rate = 0.00002\n",
    "\n",
    "        ## model, loss, and optimizer\n",
    "        if transfer:\n",
    "            model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "            model.load_state_dict(torch.load('model_H2.ckpt'))\n",
    "            model.fc1.weight.requires_grad = False\n",
    "            model.fc1.bias.requires_grad = False\n",
    "            model.fc2.weight.requires_grad = False\n",
    "            model.fc2.bias.requires_grad = False\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        else:\n",
    "            model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        ## train, val, test data split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.1, random_state=1)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "        scores_epoch = list()\n",
    "        num_epochs = 10000\n",
    "        N = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            inputs = torch.from_numpy(X_train)\n",
    "            labels = torch.from_numpy(y_train)\n",
    "\n",
    "            outputs = model(inputs).view(-1,)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            N += 1\n",
    "            if epoch % 5 == 0:\n",
    "                inputs_val = torch.from_numpy(X_val)\n",
    "                labels_val = torch.from_numpy(y_val)\n",
    "\n",
    "                outputs_val = model(inputs_val).view(-1,)\n",
    "                score = r2_score(labels_val.data.numpy(), outputs_val.data.numpy())\n",
    "        #         print('Predictive accuracy on validation set at epoch {}/{} is {}'.format(epoch, num_epochs, score)) \n",
    "                scores_epoch.append(score)\n",
    "            if len(scores_epoch) >= 2:\n",
    "                if score < scores_epoch[-2]:\n",
    "                    break\n",
    "        scores_epochs.append(scores_epoch)\n",
    "        Ns.append(N)\n",
    "    \n",
    "        score_train = r2_score(torch.from_numpy(y_train).data.numpy(), model(torch.from_numpy(X_train)).view(-1,).data.numpy())     \n",
    "#         score_train = mean_squared_error(torch.from_numpy(y_train).data.numpy(), model(torch.from_numpy(X_train)).view(-1,).data.numpy())     \n",
    "        scores_train.append(score_train)\n",
    "    \n",
    "        pred_tests.append(model(torch.from_numpy(X_test)).view(-1,).data.numpy())\n",
    "        grt_train_X.append(torch.from_numpy(X_train).data.numpy())\n",
    "        grt_test_X.append(torch.from_numpy(X_test).data.numpy())\n",
    "        grt_tests.append(torch.from_numpy(y_test).data.numpy())\n",
    "        score_test = r2_score(torch.from_numpy(y_test).data.numpy(), model(torch.from_numpy(X_test)).view(-1,).data.numpy())\n",
    "#         score_test = mean_squared_error(torch.from_numpy(y_test).data.numpy(), model(torch.from_numpy(X_test)).view(-1,).data.numpy())\n",
    "        scores_test.append(score_test)\n",
    "        torch.save(model, f'{name}.pt')\n",
    "    return scores_train, scores_test, grt_train_X, grt_test_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:09:00<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_train_H2_130K_wo_transfer, scores_test_H2_130K_wo_transfer, grt_train_X_wo_transfer, grt_test_X_wo_transfer = transfer_learning(transfer=False, nsamples=100, nbatches=1000,\"One\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [35:26<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_train_H2_130K_w_transfer, scores_test_H2_130K_w_transfer, grt_train_X_w_transfer, grt_test_X_w_transfer = transfer_learning(transfer=True, nsamples=100, nbatches=1000,\"Two\")     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
