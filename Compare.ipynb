{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sherpa \n",
    "parameters = [sherpa.Continuous(name='lr', range=[0.000001, 0.1], scale='log'),\n",
    "              sherpa.Ordinal(name='Batch ', range=[16, 32, 64]),\n",
    "              sherpa.Discrete(name='H_l1', range=[10, 300]),\n",
    "              sherpa.Discrete(name='H_l2', range=[5, 150]),\n",
    "              sherpa.Choice(name='activation', range=[\"nn.Hardswish\",'nn.PReLU', 'nn.ReLU', 'nn.Sigmoid',\"nn.LeakyReLU\"])]\n",
    "algorithm = sherpa.algorithms.RandomSearch(max_num_trials=150)\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                 algorithm=algorithm,\n",
    "                 lower_is_better=False,\n",
    "                 disable_dashboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in study:\n",
    "    print(trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NeuralNet_sherpa_optimize' from 'transfer_learning' (c:\\Users\\gcooper2\\Downloads\\TL_MOF-master\\TL_MOF-master\\transfer_learning.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9876/2991819766.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msherpa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# help function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransfer_learning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeuralNet_sherpa_optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataset_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_descriptors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_scaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NeuralNet_sherpa_optimize' from 'transfer_learning' (c:\\Users\\gcooper2\\Downloads\\TL_MOF-master\\TL_MOF-master\\transfer_learning.py)"
     ]
    }
   ],
   "source": [
    "import sherpa \n",
    "# help function\n",
    "from transfer_learning import NeuralNet_sherpa_optimize\n",
    "from dataset_loader import data_loader, all_filter, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "parameters = [sherpa.Continuous(name='lr', range=[0.000001, 0.1], scale='log'),\n",
    "              sherpa.Discrete(name='Epoch', range=[1000,100000]),\n",
    "              sherpa.Discrete(name='H_l1', range=[10, 300]),\n",
    "              sherpa.Discrete(name='H_l2', range=[5, 150]),\n",
    "              sherpa.Choice(name='activate', range=[\"nn.Hardswish\",'nn.PReLU', 'nn.ReLU', 'nn.Sigmoid',\"nn.LeakyReLU\"])]\n",
    "algorithm = sherpa.algorithms.RandomSearch(max_num_trials=150)\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                 algorithm=algorithm,\n",
    "                 lower_is_better=False,\n",
    "                 disable_dashboard=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = os.getcwd()\n",
    "file_name = 'data/CrystGrowthDesign_SI.csv'\n",
    "\n",
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = ['void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest']\n",
    "one_filter_columns = ['H2@100 bar/243K (wt%)'] \n",
    "another_filter_columns = ['H2@100 bar/130K (wt%)'] \n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "\n",
    "# extract descriptors and gas adsorptions\n",
    "one_property = one_filter(data, one_filter_columns)\n",
    "descriptors = get_descriptors(data, descriptor_columns)\n",
    "\n",
    "# prepare training inputs and outputs\n",
    "X = np.array(descriptors.values, dtype=np.float32)\n",
    "y = np.array(one_property.values, dtype=np.float32).reshape(len(X), )\n",
    "X = data_scaler(X)\n",
    "y = data_scaler(y.reshape(-1, 1)).reshape(len(X),)\n",
    "\n",
    "for trial in study:\n",
    "    learning_rate=trial.parameters[\"lr\"]\n",
    "    batch=trial.parameters[\"Epoch\"]\n",
    "    ## model, loss, and optimizer\n",
    "    # always used 5 features to make 1 prediction hence 5,1\n",
    "    model = NeuralNet_sh(5,1,trial.parameters).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    ## train, val, test data split\n",
    "    # note these are not split by cluster yet.....\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Training\n",
    "    scores_epochs = list()\n",
    "    num_epochs = batch\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        inputs = torch.from_numpy(X_train)\n",
    "        labels = torch.from_numpy(y_train)\n",
    "\n",
    "        outputs = model(inputs).view(-1,)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 5 == 0:\n",
    "            inputs_val = torch.from_numpy(X_val)\n",
    "            labels_val = torch.from_numpy(y_val)\n",
    "\n",
    "            outputs_val = model(inputs_val).view(-1,)\n",
    "            score = r2_score(labels_val.data.numpy(), outputs_val.data.numpy())\n",
    "    #         print('Predictive accuracy on validation set at epoch {}/{} is {}'.format(epoch, num_epochs, score)) \n",
    "            scores_epochs.append(score)\n",
    "        score_test = r2_score(torch.from_numpy(y_test).data.numpy(), model(torch.from_numpy(X_test)).view(-1,).data.numpy())\n",
    "        score_train = r2_score(torch.from_numpy(y_train).data.numpy(), model(torch.from_numpy(X_test)).view(-1,).data.numpy())\n",
    "        study.add_observation(trial=trial,\n",
    "                            objective=score_test,\n",
    "                            context={'training_error': score_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 5.807819413186732e-06,\n",
       " 'Epoch ': 31789,\n",
       " 'H_l1': 299,\n",
       " 'H_l2': 41,\n",
       " 'activate': 'nn.LeakyReLU'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NeuralNet',\n",
       " 'Variable',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'division',\n",
       " 'lr_scheduler',\n",
       " 'nn',\n",
       " 'np',\n",
       " 'optim',\n",
       " 'os',\n",
       " 'plt',\n",
       " 'print_function',\n",
       " 'time',\n",
       " 'torch']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transfer_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08e68b6ed4c3e6ab7f2854c6050494f7e80cd03b8cabcf5e57c6479ed706d2af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Transfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
