{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "/Users/gregcooper/Desktop/Research/TL_MOF/Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n"
     ]
    }
   ],
   "source": [
    "# help function\n",
    "from transfer_learning import NeuralNet\n",
    "from dataset_loader import data_loader, all_filter, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# file name and data path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = os.getcwd()\n",
    "file_name = 'data/CrystGrowthDesign_SI.csv'\n",
    "\n",
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = ['void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest']\n",
    "one_filter_columns = ['H2@100 bar/243K (wt%)'] \n",
    "another_filter_columns = ['H2@100 bar/130K (wt%)'] \n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "data=data.reset_index(drop=True)\n",
    "# extract descriptors and gas adsorptions\n",
    "one_property = one_filter(data, one_filter_columns)\n",
    "descriptors = get_descriptors(data, descriptor_columns)\n",
    "\n",
    "# prepare training inputs and outputs\n",
    "X = np.array(descriptors.values, dtype=np.float32)\n",
    "y = np.array(one_property.values, dtype=np.float32).reshape(len(X), )\n",
    "\n",
    "X = data_scaler(X)\n",
    "y = data_scaler(y.reshape(-1, 1)).reshape(len(X),)\n",
    "epochs=1000\n",
    "## hyper-parameters\n",
    "\n",
    "seeds = np.arange(epochs)\n",
    "Ns = list()\n",
    "scores_epochs = list()\n",
    "scores_test = list()\n",
    "scores_train = list()\n",
    "\n",
    "pred_tests = list()\n",
    "grt_train_X = list()\n",
    "grt_test_X = list()\n",
    "grt_tests = list()\n",
    "nsamples=100\n",
    "\n",
    "input_size = 6\n",
    "hidden_size_1 = 128\n",
    "hidden_size_2 = 64\n",
    "output_size = 1\n",
    "learning_rate = .002\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "from Statistics_helper import stratified_cluster_sample\n",
    "df,t_1,t_2,y_1,y_2=stratified_cluster_sample(1,data,descriptor_columns,one_filter_columns[0],5,net_out=True)\n",
    "df=df[0]\n",
    "#df=df.drop(\"Cluster\",axis=1)\n",
    "interest=one_filter_columns[0]\n",
    "descriptor_columns.append(\"Cluster\")\n",
    "features=descriptor_columns\n",
    "from transfer_learning import MyDataset\n",
    "df_train,df_val,y_df_train,y_df_val = train_test_split(df[features],df[interest],test_size=.1)\n",
    "df_train[interest]=np.array(y_df_train)\n",
    "df_val[interest]=(np.array(y_df_val))\n",
    "first=MyDataset(df_train,interest,features)\n",
    "train_loader= torch.utils.data.DataLoader(first,batch_size=100)\n",
    "second=MyDataset(df_val,interest,features)\n",
    "val_loader = torch.utils.data.DataLoader(second,batch_size=len(df_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 : 122 - batch loss: 0.023208312690258026, lr: 0.002\n",
      "Epoch 2/100 : 244 - batch loss: 0.023299917578697205, lr: 0.002\n",
      "Epoch 3/100 : 366 - batch loss: 0.023600302636623383, lr: 0.002\n",
      "Epoch 4/100 : 488 - batch loss: 0.02284274250268936, lr: 0.002\n",
      "Epoch 5/100 : 610 - batch loss: 0.023304825648665428, lr: 0.002\n",
      "Epoch 6/100 : 732 - batch loss: 0.02345854416489601, lr: 0.002\n",
      "Epoch 7/100 : 854 - batch loss: 0.022728588432073593, lr: 0.002\n",
      "Epoch 8/100 : 976 - batch loss: 0.023444347083568573, lr: 0.002\n",
      "Epoch 9/100 : 1098 - batch loss: 0.02291264757514, lr: 0.002\n",
      "Epoch 10/100 : 1220 - batch loss: 0.022849127650260925, lr: 0.002\n",
      "Epoch 11/100 : 1342 - batch loss: 0.023672999814152718, lr: 0.002\n",
      "Epoch 12/100 : 1464 - batch loss: 0.022969074547290802, lr: 0.002\n",
      "Epoch 13/100 : 1586 - batch loss: 0.023304585367441177, lr: 0.002\n",
      "Epoch 14/100 : 1708 - batch loss: 0.02268330194056034, lr: 0.002\n",
      "Epoch 15/100 : 1830 - batch loss: 0.02322879247367382, lr: 0.002\n",
      "Epoch 16/100 : 1952 - batch loss: 0.023500517010688782, lr: 0.002\n",
      "Epoch 17/100 : 2074 - batch loss: 0.022949734702706337, lr: 0.002\n",
      "Epoch 18/100 : 2196 - batch loss: 0.02378668636083603, lr: 0.002\n",
      "Epoch 19/100 : 2318 - batch loss: 0.022707724943757057, lr: 0.002\n",
      "Epoch 20/100 : 2440 - batch loss: 0.02260240726172924, lr: 0.002\n",
      "Epoch 21/100 : 2562 - batch loss: 0.02252192422747612, lr: 0.002\n",
      "Epoch 22/100 : 2684 - batch loss: 0.02254079096019268, lr: 0.002\n",
      "Epoch 23/100 : 2806 - batch loss: 0.02256029099225998, lr: 0.002\n",
      "Epoch 24/100 : 2928 - batch loss: 0.022747425362467766, lr: 0.002\n",
      "Epoch 25/100 : 3050 - batch loss: 0.02246728353202343, lr: 0.002\n",
      "Epoch 26/100 : 3172 - batch loss: 0.022765766829252243, lr: 0.002\n",
      "Epoch 27/100 : 3294 - batch loss: 0.02258407138288021, lr: 0.002\n",
      "Epoch 28/100 : 3416 - batch loss: 0.02247285470366478, lr: 0.002\n",
      "Epoch 29/100 : 3538 - batch loss: 0.02205638960003853, lr: 0.002\n",
      "Epoch 30/100 : 3660 - batch loss: 0.022536175325512886, lr: 0.002\n",
      "Epoch 31/100 : 3782 - batch loss: 0.02216131053864956, lr: 0.002\n",
      "Epoch 32/100 : 3904 - batch loss: 0.022035134956240654, lr: 0.002\n",
      "Epoch 33/100 : 4026 - batch loss: 0.022511763498187065, lr: 0.002\n",
      "Epoch 34/100 : 4148 - batch loss: 0.021924734115600586, lr: 0.002\n",
      "Epoch 35/100 : 4270 - batch loss: 0.02203872799873352, lr: 0.002\n",
      "Epoch 36/100 : 4392 - batch loss: 0.0221533365547657, lr: 0.002\n",
      "Epoch 37/100 : 4514 - batch loss: 0.022559048607945442, lr: 0.002\n",
      "Epoch 38/100 : 4636 - batch loss: 0.02237897925078869, lr: 0.002\n",
      "Epoch 39/100 : 4758 - batch loss: 0.02302686497569084, lr: 0.002\n",
      "Epoch 40/100 : 4880 - batch loss: 0.022584041580557823, lr: 0.002\n",
      "Epoch 41/100 : 5002 - batch loss: 0.023148128762841225, lr: 0.002\n",
      "Epoch 42/100 : 5124 - batch loss: 0.02291686274111271, lr: 0.002\n",
      "Epoch 43/100 : 5246 - batch loss: 0.023628998547792435, lr: 0.002\n",
      "Epoch 44/100 : 5368 - batch loss: 0.022962702438235283, lr: 0.002\n",
      "Epoch 45/100 : 5490 - batch loss: 0.023745236918330193, lr: 0.002\n",
      "Epoch 46/100 : 5612 - batch loss: 0.02316407859325409, lr: 0.002\n",
      "Epoch 47/100 : 5734 - batch loss: 0.02229686826467514, lr: 0.002\n",
      "Epoch 48/100 : 5856 - batch loss: 0.022332698106765747, lr: 0.002\n",
      "Epoch 49/100 : 5978 - batch loss: 0.022907868027687073, lr: 0.002\n",
      "Epoch 50/100 : 6100 - batch loss: 0.022333180531859398, lr: 0.002\n",
      "Epoch 51/100 : 6222 - batch loss: 0.0216736551374197, lr: 0.002\n",
      "Epoch 52/100 : 6344 - batch loss: 0.021953755989670753, lr: 0.002\n",
      "Epoch 53/100 : 6466 - batch loss: 0.021996496245265007, lr: 0.002\n",
      "Epoch 54/100 : 6588 - batch loss: 0.022227423265576363, lr: 0.002\n",
      "Epoch 55/100 : 6710 - batch loss: 0.022208360955119133, lr: 0.002\n",
      "Epoch 56/100 : 6832 - batch loss: 0.021241700276732445, lr: 0.002\n",
      "Epoch 57/100 : 6954 - batch loss: 0.021775567904114723, lr: 0.002\n",
      "Epoch 58/100 : 7076 - batch loss: 0.022118523716926575, lr: 0.002\n",
      "Epoch 59/100 : 7198 - batch loss: 0.021987052634358406, lr: 0.002\n",
      "Epoch 60/100 : 7320 - batch loss: 0.022009925916790962, lr: 0.002\n",
      "Epoch 61/100 : 7442 - batch loss: 0.02162070944905281, lr: 0.002\n",
      "Epoch 62/100 : 7564 - batch loss: 0.02221975289285183, lr: 0.002\n",
      "Epoch 63/100 : 7686 - batch loss: 0.021297907456755638, lr: 0.002\n",
      "Epoch 64/100 : 7808 - batch loss: 0.021714836359024048, lr: 0.002\n",
      "Epoch 65/100 : 7930 - batch loss: 0.021892284974455833, lr: 0.002\n",
      "Epoch 66/100 : 8052 - batch loss: 0.021862590685486794, lr: 0.002\n",
      "Epoch 67/100 : 8174 - batch loss: 0.021140199154615402, lr: 0.002\n",
      "Epoch 68/100 : 8296 - batch loss: 0.022001801058650017, lr: 0.002\n",
      "Epoch 69/100 : 8418 - batch loss: 0.02202337048947811, lr: 0.002\n",
      "Epoch 70/100 : 8540 - batch loss: 0.021405400708317757, lr: 0.002\n",
      "Epoch 71/100 : 8662 - batch loss: 0.021949445828795433, lr: 0.002\n",
      "Epoch 72/100 : 8784 - batch loss: 0.021924395114183426, lr: 0.002\n",
      "Epoch 73/100 : 8906 - batch loss: 0.022190414369106293, lr: 0.002\n",
      "Epoch 74/100 : 9028 - batch loss: 0.021443650126457214, lr: 0.002\n",
      "Epoch 75/100 : 9150 - batch loss: 0.021993860602378845, lr: 0.002\n",
      "Epoch 76/100 : 9272 - batch loss: 0.021702470257878304, lr: 0.002\n",
      "Epoch 77/100 : 9394 - batch loss: 0.021716946735978127, lr: 0.002\n",
      "Epoch 78/100 : 9516 - batch loss: 0.02141893096268177, lr: 0.002\n",
      "Epoch 79/100 : 9638 - batch loss: 0.0215771347284317, lr: 0.002\n",
      "Epoch 80/100 : 9760 - batch loss: 0.021538740023970604, lr: 0.002\n",
      "Epoch 81/100 : 9882 - batch loss: 0.021519633010029793, lr: 0.002\n",
      "Epoch 82/100 : 10004 - batch loss: 0.021755803376436234, lr: 0.002\n",
      "Epoch 83/100 : 10126 - batch loss: 0.021723732352256775, lr: 0.002\n",
      "Epoch 84/100 : 10248 - batch loss: 0.022026799619197845, lr: 0.002\n",
      "Epoch 85/100 : 10370 - batch loss: 0.021479399874806404, lr: 0.002\n",
      "Epoch 86/100 : 10492 - batch loss: 0.02143126167356968, lr: 0.002\n",
      "Epoch 87/100 : 10614 - batch loss: 0.02160082384943962, lr: 0.002\n",
      "Epoch 88/100 : 10736 - batch loss: 0.0214212778955698, lr: 0.002\n",
      "Epoch 89/100 : 10858 - batch loss: 0.02106345072388649, lr: 0.002\n",
      "Epoch 90/100 : 10980 - batch loss: 0.022136051207780838, lr: 0.002\n",
      "Epoch 91/100 : 11102 - batch loss: 0.021655185148119926, lr: 0.002\n",
      "Epoch 92/100 : 11224 - batch loss: 0.02158469706773758, lr: 0.002\n",
      "Epoch 93/100 : 11346 - batch loss: 0.021372612565755844, lr: 0.002\n",
      "Epoch 94/100 : 11468 - batch loss: 0.021627938374876976, lr: 0.002\n",
      "Epoch 95/100 : 11590 - batch loss: 0.022026127204298973, lr: 0.002\n",
      "Epoch 96/100 : 11712 - batch loss: 0.02143477462232113, lr: 0.002\n",
      "Epoch 97/100 : 11834 - batch loss: 0.021304409950971603, lr: 0.002\n",
      "Epoch 98/100 : 11956 - batch loss: 0.021361086517572403, lr: 0.002\n",
      "Epoch 99/100 : 12078 - batch loss: 0.02128964103758335, lr: 0.002\n",
      "Epoch 100/100 : 12200 - batch loss: 0.02078229933977127, lr: 0.002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 12200\n",
       "\tepoch: 100\n",
       "\tepoch_length: 122\n",
       "\tmax_epochs: 100\n",
       "\tmax_iters: <class 'NoneType'>\n",
       "\toutput: 0.02078229933977127\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, MeanSquaredError\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "#declared model above\n",
    "def train_step(engine, batch):\n",
    "    x, y = batch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    y = y.view(-1,1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "def log_training(engine):\n",
    "    batch_loss = engine.state.output\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    e = engine.state.epoch\n",
    "    n = engine.state.max_epochs\n",
    "    i = engine.state.iteration\n",
    "    print(f\"Epoch {e}/{n} : {i} - batch loss: {batch_loss}, lr: {lr}\")\n",
    "\n",
    "\n",
    "trainer.run(train_loader, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument max_epochs should be larger than the start epoch defined in the state: 1 vs 100. Please, set engine.state.max_epochs = None before calling engine.run() in order to restart the training from the beginning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      4\u001b[0m train_loader\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(first,batch_size\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Transfer/lib/python3.9/site-packages/ignite/engine/engine.py:667\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, max_iters, epoch_length)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_epochs \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch:\n\u001b[0;32m--> 667\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    668\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument max_epochs should be larger than the start epoch \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    669\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefined in the state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    670\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, set engine.state.max_epochs = None \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore calling engine.run() in order to restart the training from the beginning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m=\u001b[39m max_epochs\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument max_epochs should be larger than the start epoch defined in the state: 1 vs 100. Please, set engine.state.max_epochs = None before calling engine.run() in order to restart the training from the beginning."
     ]
    }
   ],
   "source": [
    "for i in range(1,int(len(first)/2),10):\n",
    "    i=int(i)\n",
    "    print(i)\n",
    "    train_loader= torch.utils.data.DataLoader(first,batch_size=i)\n",
    "    trainer.run(train_loader, max_epochs=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.015750574\n",
      "tensor(24.1117, grad_fn=<MseLossBackward0>)\n",
      "0.9987069128425884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregcooper/opt/anaconda3/envs/Transfer/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1351])) that is different to the input size (torch.Size([1351, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc3d02422e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "start=0\n",
    "for a,b in val_loader:\n",
    "    out=model(a)\n",
    "    print(start)\n",
    "print(mean_squared_error(out.detach(),b))\n",
    "print(criterion(out,b))\n",
    "print(r2_score(out.detach().numpy(),b))\n",
    "plt.scatter(out.detach().numpy(),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregcooper/opt/anaconda3/envs/Transfer/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1351])) that is different to the input size (torch.Size([1351, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.8381, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(out,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08e68b6ed4c3e6ab7f2854c6050494f7e80cd03b8cabcf5e57c6479ed706d2af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Transfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
