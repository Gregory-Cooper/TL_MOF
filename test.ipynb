{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n"
     ]
    }
   ],
   "source": [
    "# help function\n",
    "from transfer_learning import NeuralNet\n",
    "from dataset_loader import data_loader, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# New\n",
    "from transfer_learning import MyDataset\n",
    "from Statistics_helper import stratified_cluster_sample\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "import time\n",
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "\n",
    "# file name and data path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_path = os.getcwd()\n",
    "file_name = \"data/CrystGrowthDesign_SI.csv\"\n",
    "\n",
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = [\n",
    "    \"void fraction\",\n",
    "    \"Vol. S.A.\",\n",
    "    \"Grav. S.A.\",\n",
    "    \"Pore diameter Limiting\",\n",
    "    \"Pore diameter Largest\",\n",
    "]\n",
    "one_filter_columns = [\"H2@100 bar/243K (wt%)\"]\n",
    "another_filter_columns = [\"H2@100 bar/130K (wt%)\"]\n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "data = data.reset_index(drop=True)\n",
    "# extract descriptors and gas adsorptions\n",
    "one_property = one_filter(data, one_filter_columns)\n",
    "descriptors = get_descriptors(data, descriptor_columns)\n",
    "\n",
    "# parameters\n",
    "input_size = 6\n",
    "hidden_size_1 = 128\n",
    "hidden_size_2 = 64\n",
    "output_size = 1\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# file specifics \n",
    "epochs = 1\n",
    "batch_size = 1\n",
    "filename = f\"data_epochs-{epochs}_bs-{batch_size}\"\n",
    "\n",
    "#format data\n",
    "df, t_1, t_2, y_1, y_2 = stratified_cluster_sample(\n",
    "    1, data, descriptor_columns, one_filter_columns[0], 5, net_out=True\n",
    ")\n",
    "df = df[0]\n",
    "# df=df.drop(\"Cluster\",axis=1)\n",
    "interest = one_filter_columns[0]\n",
    "descriptor_columns.append(\"Cluster\")\n",
    "features = descriptor_columns\n",
    "\n",
    "df_train, df_val, y_df_train, y_df_val = train_test_split(\n",
    "    df[features], df[interest], test_size=0.1\n",
    ")\n",
    "df_train[interest] = np.array(y_df_train)\n",
    "df_val[interest] = np.array(y_df_val)\n",
    "first = MyDataset(df_train, interest, features)\n",
    "train_loader = torch.utils.data.DataLoader(first, batch_size=512)\n",
    "second = MyDataset(df_val, interest, features)\n",
    "val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "train_loss = []\n",
    "train_r_2 = []\n",
    "val_loss = []\n",
    "val_r_2 = []\n",
    "net_time = []\n",
    "#create model\n",
    "model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    x, y = batch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "# trainer.logger = setup_logger(\"Trainer\")\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=10))\n",
    "def log_training(engine):\n",
    "    batch_loss = engine.state.output\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    e = engine.state.epoch\n",
    "    n = engine.state.max_epochs\n",
    "    i = engine.state.iteration\n",
    "    # print(f\"Epoch {e}/{n} : {i} - batch loss: {batch_loss}, lr: {lr}\")\n",
    "\n",
    "\n",
    "metrics = {\"loss\": Loss(criterion), \"r_2\": R2Score()}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "# train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "validation_evaluator = create_supervised_evaluator(\n",
    "    model, metrics=metrics, device=device\n",
    ")\n",
    "# validation_evaluator.logger = setup_logger(\"Val Evaluator\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def store_metrics(engine):\n",
    "    train_evaluator.run(train_loader)\n",
    "    validation_evaluator.run(val_loader)\n",
    "    out = train_evaluator.state.metrics\n",
    "    out_2 = validation_evaluator.state.metrics\n",
    "    end = time.time()\n",
    "    train_loss.append(out[\"loss\"])\n",
    "    train_r_2.append(out[\"r_2\"])\n",
    "    val_loss.append(out_2[\"loss\"])\n",
    "    val_r_2.append(out_2[\"r_2\"])\n",
    "    net_time.append(end-start)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "start = time.time()\n",
    "trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "dic = {\n",
    "    \"train_loss\": train_loss,\n",
    "    \"train_r_2\": train_r_2,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"val_r_2\": val_r_2,\n",
    "    \"net_time\" : net_time,\n",
    "}\n",
    "case_metrics = pd.DataFrame.from_dict(dic)\n",
    "case_metrics.to_json(f\"{filename}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_r_2</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_r_2</th>\n",
       "      <th>net_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167616</td>\n",
       "      <td>0.985702</td>\n",
       "      <td>0.250689</td>\n",
       "      <td>0.980132</td>\n",
       "      <td>15.107011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  train_r_2  val_loss   val_r_2   net_time\n",
       "0    0.167616   0.985702  0.250689  0.980132  15.107011"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"data_epochs-1_bs-1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
