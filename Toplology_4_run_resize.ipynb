{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import get_processed_data\n",
    "from sklearn.decomposition import PCA\n",
    "from Functions import make_pca_agg_fit,add_pca_and_graph, prep_data_splits,plot_outline,plot_dendrogram,size_clusters,anaylsis\n",
    "import torch \n",
    "import os\n",
    "from transfer_learning import MyDataset,NeuralNet_sherpa_optimize\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine,Events\n",
    "import time \n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-1e8a20307f70>:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  abridge[\"t_cluster\"]=abridge[\"topology\"].map(dic)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1e8a20307f70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m#runs model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mtransfer_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mtransfer_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m             \u001b[0mtest_loss_holder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[0mtest_r2_holder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_r_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    758\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTERMINATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhandlers_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[1;31m# update time wrt handlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_fire_event\u001b[1;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mothers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mothers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mevent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_attrib_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mevent_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;31m# setup input handler as parent to make has_event_handler work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-1e8a20307f70>\u001b[0m in \u001b[0;36mstore_metrics\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                 \u001b[0mout1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mout2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mout3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\Desktop\\version_1\\trade1-repo\\TL_MOF\\transfer_learning.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Greg\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#if you want certain output and in order of array\n",
    "    # controls graph pca and make pca\n",
    "    # graph of pca cluster annotated\n",
    "    # is graph of median topologies and dendrogram cluster graph\n",
    "    # is the epoch count \n",
    "    # is graphs of first learning\n",
    "    # transfer learning epochs\n",
    "t_name=\"T5\"\n",
    "loud=[0,0,0,0,0,0,0,1]\n",
    "#make cluster run through\n",
    "processed=get_processed_data()\n",
    "#set variablility and number of clusters\n",
    "var=.9\n",
    "g_comp=4\n",
    "#predicting (only supports 1 prediction as of now, but should work with more not tested)\n",
    "interest = [\"H2@100 bar/243K (wt%)\"]\n",
    "interest2=[\"H2@100 bar/243K (wt%)\"]\n",
    "#make PCA to run )\n",
    "#Make PCA on data and generate \n",
    "pc1,pc2,color=make_pca_agg_fit(1,processed,var,g_comp,array_out=True,loud=loud[0])\n",
    "#get unprocessed data\n",
    "data=get_processed_data(unprocessed=True)\n",
    "pca_df=add_pca_and_graph(processed,pc1,pc2,color,graph=loud[1])\n",
    "#puts in old data needed, but can't be processed in PCA\n",
    "pca_df[['MOF ID',interest[0],'topology']]=data[['MOF ID',interest[0],'topology']]\n",
    "\n",
    "#removes some unneeded columns\n",
    "abridge=pca_df[['MOF ID', 'void fraction', 'Vol. S.A.', 'Grav. S.A.','Pore diameter Limiting', 'Pore diameter Largest',interest[0],'topology', 'Pc1', 'Pc2', 'Cluster']]\n",
    "# generate holder for dataframes for looping \n",
    "if loud[2]:\n",
    "    a=abridge.groupby(\"topology\").median()[[\"Pc1\",\"Pc2\"]]\n",
    "    plt.scatter(a[\"Pc1\"],a[\"Pc2\"])\n",
    "    color = AgglomerativeClustering(n_clusters=g_comp).fit_predict(a)\n",
    "    #color=gm.predict(a)\n",
    "    plt.scatter(a[\"Pc1\"],a[\"Pc2\"],c=color)\n",
    "    plt.legend()\n",
    "    plot_outline(abridge)\n",
    "\n",
    "\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "    model = model.fit(a)\n",
    "        \n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(20)\n",
    "    f.set_figheight(20)\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    # plot the top three levels of the dendrogram\n",
    "    plot_dendrogram(model,a, truncate_mode=\"level\", p=3)\n",
    "a=abridge.groupby(\"topology\").median()[[\"Pc1\",\"Pc2\"]]\n",
    "model = AgglomerativeClustering(n_clusters=g_comp)\n",
    "model = model.fit(a)\n",
    "model.labels_\n",
    "dic={}\n",
    "for a,b in zip(a.index,model.labels_):\n",
    "    dic[a]=b\n",
    "abridge[\"t_cluster\"]=abridge[\"topology\"].map(dic)\n",
    "O_M_Cluster=[]\n",
    "for i in sorted(abridge[\"t_cluster\"].unique()):\n",
    "    Temp=abridge[abridge[\"t_cluster\"]==i]\n",
    "    O_M_Cluster.append(Temp)\n",
    "\n",
    "# parameters for neural net\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "input_size = 5\n",
    "output_size = 1\n",
    "resolution=100\n",
    "# specifc for neural net class\n",
    "trial_parameters={\n",
    "    \"lr\" : 0.005,\n",
    "    \"H_l1\" : 250,\n",
    "    \"activate\" :  \"nn.PReLU\"\n",
    "}\n",
    "resize=True\n",
    "#specify cluster\n",
    "meta_trials=100\n",
    "meta=[]\n",
    "if loud[7]:\n",
    "    counter=0\n",
    "for seed,i in enumerate(range(meta_trials)):\n",
    "    np.random.seed(seed)\n",
    "    #create holder for meta anaylsis\n",
    "    mega={}\n",
    "    if resize:\n",
    "        M_Cluster=size_clusters(O_M_Cluster)\n",
    "    else:\n",
    "        M_Cluster=O_M_Cluster\n",
    "    for i in range(len(M_Cluster)):\n",
    "        mega[i]=[]\n",
    "    for count,g in enumerate(M_Cluster):\n",
    "        cluster=count\n",
    "        data = g\n",
    "        #checks for index problem\n",
    "        data = data.reset_index(drop=True)\n",
    "        #computer requirements \n",
    "        device = torch.device(\"cpu\")\n",
    "        base_path = os.getcwd()\n",
    "\n",
    "        features = [\n",
    "            \"void fraction\",\n",
    "            \"Vol. S.A.\",\n",
    "            \"Grav. S.A.\",\n",
    "            \"Pore diameter Limiting\",\n",
    "            \"Pore diameter Largest\",\n",
    "        ]\n",
    "        #create splits \n",
    "        df_train,df_val,df_test=prep_data_splits(data,features,interest)\n",
    "        #create dataset loader objects for looping through data,default test size is .2\n",
    "        first = MyDataset(df_train, interest, features)\n",
    "        train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "\n",
    "        second = MyDataset(df_val, interest, features)\n",
    "        val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "        third = MyDataset(df_test, interest, features)\n",
    "        test_loader=torch.utils.data.DataLoader(third, batch_size=len(df_test))\n",
    "        #intialize holders for data\n",
    "        train_loss = []\n",
    "        train_r_2 = []\n",
    "        val_loss = []\n",
    "        val_r_2 = []\n",
    "        test_loss = []\n",
    "        test_r_2 = []\n",
    "        net_time = []\n",
    "\n",
    "        # initalize model\n",
    "        model = NeuralNet_sherpa_optimize(5, 1, trial_parameters).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        learning_rate = trial_parameters[\"lr\"]\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        #Create engine for running NN, pytorch\n",
    "\n",
    "        def train_step(engine, batch):\n",
    "            x, y = batch\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            return loss.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "        #sets up loss and R scores and stores values\n",
    "        #Note: note sure if can condense this into a module b/c of decorator and namespace\n",
    "        @trainer.on(Events.EPOCH_COMPLETED(every=50))\n",
    "        def store_metrics(engine):\n",
    "            end = time.time()\n",
    "            e = engine.state.epoch\n",
    "            out=float(criterion(model(train_loader.dataset.x_train),train_loader.dataset.y_train))\n",
    "            out1=float(r2_score(model(train_loader.dataset.x_train).detach().numpy(),train_loader.dataset.y_train.detach().numpy()))\n",
    "            out2=float(criterion(model(val_loader.dataset.x_train),val_loader.dataset.y_train))\n",
    "            out3=float(r2_score(model(val_loader.dataset.x_train).detach().numpy(),val_loader.dataset.y_train.detach().numpy()))\n",
    "            out4=float(criterion(model(test_loader.dataset.x_train),test_loader.dataset.y_train))\n",
    "            out5=float(r2_score(model(test_loader.dataset.x_train).detach().numpy(),test_loader.dataset.y_train.detach().numpy()))\n",
    "            train_loss.append(out)\n",
    "            train_r_2.append(out1)\n",
    "            val_loss.append(out2)\n",
    "            val_r_2.append(out3)\n",
    "            test_loss.append(out4)\n",
    "            test_r_2.append(out5)\n",
    "            net_time.append(end-start)\n",
    "            if loud[3]:\n",
    "                print(e)\n",
    "        #Creates timer and runs trainer \n",
    "        start = time.time()\n",
    "        trainer.logger.disabled=True\n",
    "        trainer.run(train_loader, max_epochs=epochs)\n",
    "        plt.show()\n",
    "\n",
    "        if loud[4]:\n",
    "            plt.plot(val_r_2)\n",
    "            plt.plot(train_r_2,label=\"t\")\n",
    "            plt.plot(test_r_2,label=\"real\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.plot(val_loss)\n",
    "            plt.plot(train_loss,label=\"t\")\n",
    "            plt.plot(test_loss,label=\"real\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        #save base model for transfer into other clusters in loop\n",
    "        torch.save(model, f\"{t_name}.ckpt\")\n",
    "        #transfer learning part\n",
    "        #stores meta data\n",
    "        test_loss_holder=[]\n",
    "        test_r2_holder=[]\n",
    "        #loops through clusters\n",
    "        for count,i in enumerate(M_Cluster):\n",
    "            train_loss = []\n",
    "            train_r_2 = []\n",
    "            val_loss = []\n",
    "            val_r_2 = []\n",
    "            test_loss = []\n",
    "            test_r_2 = []\n",
    "            net_time = []\n",
    "            #loading model again\n",
    "            model=torch.load(f\"{t_name}.ckpt\")\n",
    "            descriptor_columns = [\n",
    "                \"void fraction\",\n",
    "                \"Vol. S.A.\",\n",
    "                \"Grav. S.A.\",\n",
    "                \"Pore diameter Limiting\",\n",
    "                \"Pore diameter Largest\",\n",
    "            ]\n",
    "        #turns off model learning\n",
    "            model.fc1.weight.requires_grad = False\n",
    "            model.fc1.bias.requires_grad = False\n",
    "            model.fc2.weight.requires_grad = False\n",
    "            model.fc2.bias.requires_grad = False\n",
    "            interest=interest2\n",
    "            optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "            )\n",
    "            #Base Cluster for transfer learning \n",
    "            data = i\n",
    "            #checks for index problem\n",
    "            data = data.reset_index(drop=True)\n",
    "            df_train,df_val,df_test=prep_data_splits(data,features,interest)\n",
    "            #create dataset loader objects for looping through data,default test size is .2\n",
    "            first = MyDataset(df_train, interest, features)\n",
    "            train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "\n",
    "            second = MyDataset(df_val, interest, features)\n",
    "            val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "            third = MyDataset(df_test, interest, features)\n",
    "            test_loader=torch.utils.data.DataLoader(third, batch_size=len(df_test))\n",
    "\n",
    "            #once again making training and testing engine\n",
    "            def train_step_1(engine, batch):\n",
    "                x, y = batch\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                return loss.item()\n",
    "\n",
    "\n",
    "            transfer_trainer = Engine(train_step_1)\n",
    "            n=int(epochs/resolution)\n",
    "            @transfer_trainer.on(Events.EPOCH_COMPLETED(every=n))\n",
    "            def store_metrics(engine):\n",
    "                end = time.time()\n",
    "                e = engine.state.epoch\n",
    "                out=float(criterion(model(train_loader.dataset.x_train),train_loader.dataset.y_train))\n",
    "                out1=float(r2_score(model(train_loader.dataset.x_train).detach().numpy(),train_loader.dataset.y_train.detach().numpy()))\n",
    "                out2=float(criterion(model(val_loader.dataset.x_train),val_loader.dataset.y_train))\n",
    "                out3=float(r2_score(model(val_loader.dataset.x_train).detach().numpy(),val_loader.dataset.y_train.detach().numpy()))\n",
    "                out4=float(criterion(model(test_loader.dataset.x_train),test_loader.dataset.y_train))\n",
    "                out5=float(r2_score(model(test_loader.dataset.x_train).detach().numpy(),test_loader.dataset.y_train.detach().numpy()))\n",
    "                train_loss.append(out)\n",
    "                train_r_2.append(out1)\n",
    "                val_loss.append(out2)\n",
    "                val_r_2.append(out3)\n",
    "                test_loss.append(out4)\n",
    "                test_r_2.append(out5)\n",
    "                net_time.append(end-start)\n",
    "                if loud[5]:\n",
    "                    print(e)\n",
    "            #runs model\n",
    "            transfer_trainer.logger.disabled=True\n",
    "            transfer_trainer.run(train_loader, max_epochs=epochs)\n",
    "            test_loss_holder.append(test_loss)\n",
    "            test_r2_holder.append(test_r_2)\n",
    "        #because error tends to be low, only anaylizing r2 scores\n",
    "            mega[count].append(test_r_2)\n",
    "        #plots all clusers learning in terms of test set\n",
    "        if loud[6]:\n",
    "            plt.title(f\"base {cluster}\")\n",
    "            for count,i in enumerate(test_loss_holder):\n",
    "                plt.plot(i,label=f\"Cluser {count}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.title(f\"base {cluster}\")\n",
    "            for count,i in enumerate(test_r2_holder):\n",
    "                plt.plot(i,label=f\"Cluser {count}\")\n",
    "            plt.legend()\n",
    "    if loud[7]:\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "    meta.append(mega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561\n",
      "['tpt' 'the' 'bor' 'rhrb' 'stp' 'csq' 'sodb' 'ctn' 'ssa' 'crs'] Cluster 0\n",
      "561\n",
      "['srsb' 'tbo' 'spn'] Cluster 1\n",
      "561\n",
      "['soc' 'nbob' 'pyr' 'pth' 'iac' 'she'] Cluster 2\n",
      "561\n",
      "['bcu' 'ith' 'bcs' 'bct' 'fcu'] Cluster 3\n",
      "561\n",
      "['ocu' 'rht' 'scu' 'gar' 'flu' 'pcu' 'lvtb' 'ftw' 'reo' 'nia' 'acs'] Cluster 4\n",
      "561\n",
      "['pto' 'diab' 'ssb' 'pts' 'lcsb' 'qtz'] Cluster 5\n"
     ]
    }
   ],
   "source": [
    "for i in M_Cluster:\n",
    "    print(len(i))\n",
    "    number=i[\"t_cluster\"].unique()\n",
    "    print(i[\"topology\"].unique(),f\"Cluster {number[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable epoch_conversions is [561, 561, 561, 561]\n",
      "Variable byte is 268\n",
      "Variable epochs is 500\n",
      "Variable resolution is 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "epoch_conversions=[]\n",
    "byte=df_train.iloc[0].memory_usage()\n",
    "for i in M_Cluster:\n",
    "    epoch_conversions.append(len(i))\n",
    "outs=[epoch_conversions,byte,epochs,resolution]\n",
    "o_name=[\"epoch_conversions\",\"byte\",\"epochs\",\"resolution\"]\n",
    "for a,b in zip(outs,o_name):\n",
    "    print(f\"Variable {b} is {a}\")\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce0738356d0cddf65f63e651cde4dcb8456c8dcfca6e76913a71c9f74b06aa6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
