{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:136: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train=X_train.append(X_train_1)\n",
      "c:\\Users\\gcooper2\\Documents\\GitHub\\TL_MOF\\Statistics_helper.py:137: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_test=X_test.append(X_test_1)\n"
     ]
    }
   ],
   "source": [
    "# help function\n",
    "from transfer_learning import NeuralNet\n",
    "from dataset_loader import data_loader, all_filter, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# file name and data path\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = os.getcwd()\n",
    "file_name = 'data/CrystGrowthDesign_SI.csv'\n",
    "\n",
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = ['void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest']\n",
    "one_filter_columns = ['H2@100 bar/243K (wt%)'] \n",
    "another_filter_columns = ['H2@100 bar/130K (wt%)'] \n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "data=data.reset_index(drop=True)\n",
    "# extract descriptors and gas adsorptions\n",
    "one_property = one_filter(data, one_filter_columns)\n",
    "descriptors = get_descriptors(data, descriptor_columns)\n",
    "\n",
    "# prepare training inputs and outputs\n",
    "X = np.array(descriptors.values, dtype=np.float32)\n",
    "y = np.array(one_property.values, dtype=np.float32).reshape(len(X), )\n",
    "\n",
    "X = data_scaler(X)\n",
    "y = data_scaler(y.reshape(-1, 1)).reshape(len(X),)\n",
    "epochs=1000\n",
    "## hyper-parameters\n",
    "\n",
    "seeds = np.arange(epochs)\n",
    "Ns = list()\n",
    "scores_epochs = list()\n",
    "scores_test = list()\n",
    "scores_train = list()\n",
    "\n",
    "pred_tests = list()\n",
    "grt_train_X = list()\n",
    "grt_test_X = list()\n",
    "grt_tests = list()\n",
    "nsamples=100\n",
    "\n",
    "input_size = 6\n",
    "hidden_size_1 = 128\n",
    "hidden_size_2 = 64\n",
    "output_size = 1\n",
    "learning_rate = .0002\n",
    "\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "from Statistics_helper import stratified_cluster_sample\n",
    "df,t_1,t_2,y_1,y_2=stratified_cluster_sample(1,data,descriptor_columns,one_filter_columns[0],5,net_out=True)\n",
    "df=df[0]\n",
    "#df=df.drop(\"Cluster\",axis=1)\n",
    "interest=one_filter_columns[0]\n",
    "descriptor_columns.append(\"Cluster\")\n",
    "features=descriptor_columns\n",
    "from transfer_learning import MyDataset\n",
    "df_train,df_val,y_df_train,y_df_val = train_test_split(df[features],df[interest],test_size=.1)\n",
    "df_train[interest]=np.array(y_df_train)\n",
    "df_val[interest]=(np.array(y_df_val))\n",
    "first=MyDataset(df_train,interest,features)\n",
    "train_loader= torch.utils.data.DataLoader(first,batch_size=512)\n",
    "second=MyDataset(df_val,interest,features)\n",
    "val_loader = torch.utils.data.DataLoader(second,batch_size=len(df_val))\n",
    "\n",
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, MeanSquaredError\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "#declared model above\n",
    "def train_step(engine, batch):\n",
    "    x, y = batch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "def log_training(engine):\n",
    "    batch_loss = engine.state.output\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    e = engine.state.epoch\n",
    "    n = engine.state.max_epochs\n",
    "    i = engine.state.iteration\n",
    "    print(f\"Epoch {e}/{n} : {i} - batch loss: {batch_loss}, lr: {lr}\")\n",
    "import time\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer\n",
    "from ignite.handlers import Checkpoint\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.utils import setup_logger\n",
    "from ignite.utils import setup_logger\n",
    "from ignite.contrib.handlers.clearml_logger import (\n",
    "    ClearMLLogger,\n",
    "    ClearMLSaver,\n",
    "    GradsHistHandler,\n",
    "    GradsScalarHandler,\n",
    "    WeightsHistHandler,\n",
    "    WeightsScalarHandler,\n",
    "    global_step_from_engine,\n",
    ")\n",
    "train_loss=[]\n",
    "train_r_2=[]\n",
    "val_loss=[]\n",
    "val_r_2=[]\n",
    "\n",
    "epochs=100\n",
    "batch_size=256\n",
    "filename=f\"data_epochs-{epochs}_bs-{batch_size}\"\n",
    "store_loss=[]\n",
    "store_time=[]\n",
    "start=time.time()\n",
    "model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "trainer = Engine(train_step)\n",
    "#trainer.logger = setup_logger(\"Trainer\")\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=10))\n",
    "def log_training(engine):\n",
    "    batch_loss = engine.state.output\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    e = engine.state.epoch\n",
    "    n = engine.state.max_epochs\n",
    "    i = engine.state.iteration\n",
    "    #print(f\"Epoch {e}/{n} : {i} - batch loss: {batch_loss}, lr: {lr}\")\n",
    "metrics = {\"loss\": Loss(criterion),\"r_2\" : R2Score()}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "#train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "#validation_evaluator.logger = setup_logger(\"Val Evaluator\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def store_metrics(engine):\n",
    "    train_evaluator.run(train_loader)\n",
    "    validation_evaluator.run(val_loader)\n",
    "    out=train_evaluator.state.metrics\n",
    "    out_2=validation_evaluator.state.metrics\n",
    "    train_loss.append(out[\"loss\"])\n",
    "    train_r_2.append(out[\"r_2\"])\n",
    "    val_loss.append(out_2[\"loss\"])\n",
    "    val_r_2.append(out_2[\"r_2\"])\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(first,batch_size=batch_size)\n",
    "trainer.run(train_loader, max_epochs=epochs)\n",
    "dic={\"train_loss\": train_loss,\"train_r_2\": train_r_2,\"val_loss\": val_loss,\"val_r_2\": val_r_2}\n",
    "case_metrics=pd.DataFrame.from_dict(dic)\n",
    "\n",
    "\n",
    "model.fc1.weight.requires_grad = False\n",
    "model.fc1.bias.requires_grad = False\n",
    "model.fc2.weight.requires_grad = False\n",
    "model.fc2.bias.requires_grad = False\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    ")\n",
    "interest=another_filter_columns[0]\n",
    "first = MyDataset(df_train, interest, features)\n",
    "train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "second = MyDataset(df_val, interest, features)\n",
    "val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, MeanSquaredError\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "#declared model above\n",
    "def train_step(engine, batch):\n",
    "    x, y = batch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "def log_training(engine):\n",
    "    batch_loss = engine.state.output\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    e = engine.state.epoch\n",
    "    n = engine.state.max_epochs\n",
    "    i = engine.state.iteration\n",
    "    print(f\"Epoch {e}/{n} : {i} - batch loss: {batch_loss}, lr: {lr}\")\n",
    "\n",
    "\n",
    "#trainer.run(train_loader, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer\n",
    "from ignite.handlers import Checkpoint\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.utils import setup_logger\n",
    "from ignite.utils import setup_logger\n",
    "from ignite.contrib.handlers.clearml_logger import (\n",
    "    ClearMLLogger,\n",
    "    ClearMLSaver,\n",
    "    GradsHistHandler,\n",
    "    GradsScalarHandler,\n",
    "    WeightsHistHandler,\n",
    "    WeightsScalarHandler,\n",
    "    global_step_from_engine,\n",
    ")\n",
    "train_loss=[]\n",
    "train_r_2=[]\n",
    "val_loss=[]\n",
    "val_r_2=[]\n",
    "\n",
    "epochs=10\n",
    "batch_size=128\n",
    "filename=f\"data_epochs-{epochs}_bs-{batch_size}\"\n",
    "store_loss=[]\n",
    "store_time=[]\n",
    "start=time.time()\n",
    "model = NeuralNet(input_size, hidden_size_1, hidden_size_2, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "trainer = Engine(train_step)\n",
    "#trainer.logger = setup_logger(\"Trainer\")\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=10))\n",
    "def log_training(engine):\n",
    "    batch_loss = engine.state.output\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    e = engine.state.epoch\n",
    "    n = engine.state.max_epochs\n",
    "    i = engine.state.iteration\n",
    "    #print(f\"Epoch {e}/{n} : {i} - batch loss: {batch_loss}, lr: {lr}\")\n",
    "metrics = {\"loss\": Loss(criterion),\"r_2\" : R2Score()}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "#train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "#validation_evaluator.logger = setup_logger(\"Val Evaluator\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def store_metrics(engine):\n",
    "    train_evaluator.run(train_loader)\n",
    "    validation_evaluator.run(val_loader)\n",
    "    out=train_evaluator.state.metrics\n",
    "    out_2=validation_evaluator.state.metrics\n",
    "    train_loss.append(out[\"loss\"])\n",
    "    train_r_2.append(out[\"r_2\"])\n",
    "    val_loss.append(out_2[\"loss\"])\n",
    "    val_r_2.append(out_2[\"r_2\"])\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(first,batch_size=batch_size)\n",
    "trainer.run(train_loader, max_epochs=epochs)\n",
    "dic={\"train_loss\": train_loss,\"train_r_2\": train_r_2,\"val_loss\": val_loss,\"val_r_2\": val_r_2}\n",
    "case_metrics=pd.DataFrame.from_dict(dic)\n",
    "case_metrics.to_json(f\"{filename}.json\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08e68b6ed4c3e6ab7f2854c6050494f7e80cd03b8cabcf5e57c6479ed706d2af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Transfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
