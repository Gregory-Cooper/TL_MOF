{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help function\n",
    "from transfer_learning import NeuralNet_sherpa_optimize\n",
    "from dataset_loader import data_loader, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# New\n",
    "from transfer_learning import MyDataset\n",
    "from Statistics_helper import stratified_cluster_sample\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "import time\n",
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "import sherpa\n",
    "from sklearn.metrics import r2_score\n",
    "parameters = [\n",
    "    sherpa.Continuous(name=\"lr\", range=[0.0002, .1] ),\n",
    "    # sherpa.Discrete(name='Epoch', range=[10,100]),\n",
    "    sherpa.Discrete(name=\"H_l1\", range=[10, 400]),\n",
    "    sherpa.Choice(\n",
    "        name=\"activate\",\n",
    "        range=[\"nn.Hardswish\", \"nn.PReLU\", \"nn.ReLU\", \"nn.Sigmoid\", \"nn.LeakyReLU\"],\n",
    "    ),\n",
    "]\n",
    "algorithm = sherpa.algorithms.RandomSearch(max_num_trials=1)\n",
    "study = sherpa.Study(\n",
    "    parameters=parameters,\n",
    "    algorithm=algorithm,\n",
    "    lower_is_better=False,\n",
    "    disable_dashboard=True,\n",
    ")\n",
    "\n",
    "# file name and data path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_path = os.getcwd()\n",
    "file_name = \"data/CrystGrowthDesign_SI.csv\"\n",
    "\n",
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = [\n",
    "    \"void fraction\",\n",
    "    \"Vol. S.A.\",\n",
    "    \"Grav. S.A.\",\n",
    "    \"Pore diameter Limiting\",\n",
    "    \"Pore diameter Largest\",\n",
    "]\n",
    "one_filter_columns = [\"H2@100 bar/243K (wt%)\"]\n",
    "another_filter_columns = [\"H2@100 bar/130K (wt%)\"]\n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "data = data.reset_index(drop=True)\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "# parameters\n",
    "input_size = 5\n",
    "output_size = 1\n",
    "\n",
    "# file specifics \n",
    "#filename = f\"data_epochs-{epochs}_bs-{batch_size}\"\n",
    "\n",
    "#format data\n",
    "for trial in study:\n",
    "    learning_rate = trial.parameters[\"lr\"]\n",
    "    df, t_1, t_2, y_1, y_2 = stratified_cluster_sample(\n",
    "        1, data, descriptor_columns, one_filter_columns[0], 5, net_out=True\n",
    "    )\n",
    "    df = df[0]\n",
    "    df=df.drop(\"Cluster\",axis=1)\n",
    "    interest = one_filter_columns[0]\n",
    "    #descriptor_columns.append(\"Cluster\")\n",
    "    features = descriptor_columns\n",
    "\n",
    "    df_train, df_val, y_df_train, y_df_val = train_test_split(\n",
    "        df[features], df[interest], test_size=0.1\n",
    "    )\n",
    "    df_train[interest] = np.array(y_df_train)\n",
    "    df_val[interest] = np.array(y_df_val)\n",
    "    first = MyDataset(df_train, interest, features)\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    second = MyDataset(df_val, interest, features)\n",
    "    val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "    train_loss = []\n",
    "    train_r_2 = []\n",
    "    val_loss = []\n",
    "    val_r_2 = []\n",
    "    net_time = []\n",
    "    #create model\n",
    "    model = NeuralNet_sherpa_optimize(5, 1, trial.parameters).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def train_step(engine, batch):\n",
    "        x, y = batch\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    trainer = Engine(train_step)\n",
    "\n",
    "\n",
    "    metrics = {\"loss\": Loss(criterion), \"r_2\": R2Score()}\n",
    "\n",
    "    #train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "    # train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "    #validation_evaluator = create_supervised_evaluator(\n",
    "    #    model, metrics=metrics, device=device\n",
    "    #)\n",
    "    # validation_evaluator.logger = setup_logger(\"Val Evaluator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size,shuffle=True)\n",
    "    start = time.time()\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "\n",
    "    descriptor_columns = [\n",
    "        \"void fraction\",\n",
    "        \"Vol. S.A.\",\n",
    "        \"Grav. S.A.\",\n",
    "        \"Pore diameter Limiting\",\n",
    "        \"Pore diameter Largest\",\n",
    "    ]\n",
    "    model.fc1.weight.requires_grad = False\n",
    "    model.fc1.bias.requires_grad = False\n",
    "    model.fc2.weight.requires_grad = False\n",
    "    model.fc2.bias.requires_grad = False\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "    df, t_1, t_2, y_1, y_2 = stratified_cluster_sample(\n",
    "        1, data, descriptor_columns, another_filter_columns[0], 5, net_out=True\n",
    "    )\n",
    "    df = df[0]\n",
    "    df=df.drop(\"Cluster\",axis=1)\n",
    "    interest = another_filter_columns[0]\n",
    "    #descriptor_columns.append(\"Cluster\")\n",
    "    features = descriptor_columns\n",
    "\n",
    "    df_train, df_val, y_df_train, y_df_val = train_test_split(\n",
    "        df[features], df[interest], test_size=0.1\n",
    "    )\n",
    "    df_train[interest] = np.array(y_df_train)\n",
    "    df_val[interest] = np.array(y_df_val)\n",
    "    first = MyDataset(df_train, interest, features)\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    second = MyDataset(df_val, interest, features)\n",
    "    val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "    interest=another_filter_columns[0]\n",
    "    first = MyDataset(df_train, interest, features)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    second = MyDataset(df_val, interest, features)\n",
    "    val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "\n",
    "    def train_step_1(engine, batch):\n",
    "        x, y = batch\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    transfer_trainer = Engine(train_step_1)\n",
    "    metrics = {\"loss\": Loss(criterion), \"r_2\": R2Score()}\n",
    "    @transfer_trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "    def store_metrics(engine):\n",
    "        end = time.time()\n",
    "        e = engine.state.epoch\n",
    "        out=float(criterion(model(train_loader.dataset.x_train),train_loader.dataset.y_train))\n",
    "        out1=float(r2_score(model(train_loader.dataset.x_train).detach().numpy(),train_loader.dataset.y_train.detach().numpy()))\n",
    "        out2=float(criterion(model(val_loader.dataset.x_train),val_loader.dataset.y_train))\n",
    "        out3=float(r2_score(model(val_loader.dataset.x_train).detach().numpy(),val_loader.dataset.y_train.detach().numpy()))\n",
    "        train_loss.append(out)\n",
    "        train_r_2.append(out1)\n",
    "        val_loss.append(out2)\n",
    "        val_r_2.append(out3)\n",
    "        net_time.append(end-start)\n",
    "        study_dic = {\n",
    "        \"train_loss\": out,\n",
    "        \"train_r_2\": out1,\n",
    "        #\"val_loss\": out2,\n",
    "        \"val_r_2\": out3,\n",
    "        \"net_time\" : end-start,\n",
    "        }\n",
    "        study.add_observation(\n",
    "            trial=trial,iteration=e, objective=out2, context=study_dic\n",
    "        )\n",
    "        if e == transfer_trainer.state.max_epochs:\n",
    "            study.finalize(trial)\n",
    "    transfer_trainer.logger.disabled=True\n",
    "    transfer_trainer.run(train_loader, max_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.results.to_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial-ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>H_l1</th>\n",
       "      <th>activate</th>\n",
       "      <th>lr</th>\n",
       "      <th>Objective</th>\n",
       "      <th>net_time</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_r_2</th>\n",
       "      <th>val_r_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.135646</td>\n",
       "      <td>2.704047</td>\n",
       "      <td>0.129523</td>\n",
       "      <td>0.996108</td>\n",
       "      <td>0.995575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>2</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.122760</td>\n",
       "      <td>2.871803</td>\n",
       "      <td>0.117678</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>0.995984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.119683</td>\n",
       "      <td>3.023990</td>\n",
       "      <td>0.115277</td>\n",
       "      <td>0.996512</td>\n",
       "      <td>0.996075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>4</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.117104</td>\n",
       "      <td>3.170988</td>\n",
       "      <td>0.113159</td>\n",
       "      <td>0.996581</td>\n",
       "      <td>0.996165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>5</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.115549</td>\n",
       "      <td>3.329441</td>\n",
       "      <td>0.112192</td>\n",
       "      <td>0.996616</td>\n",
       "      <td>0.996222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>6</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.113456</td>\n",
       "      <td>3.504017</td>\n",
       "      <td>0.109955</td>\n",
       "      <td>0.996704</td>\n",
       "      <td>0.996313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>7</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.112190</td>\n",
       "      <td>3.649810</td>\n",
       "      <td>0.108733</td>\n",
       "      <td>0.996755</td>\n",
       "      <td>0.996368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>8</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.111470</td>\n",
       "      <td>3.817906</td>\n",
       "      <td>0.108536</td>\n",
       "      <td>0.996757</td>\n",
       "      <td>0.996387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>9</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.112155</td>\n",
       "      <td>3.968340</td>\n",
       "      <td>0.109908</td>\n",
       "      <td>0.996706</td>\n",
       "      <td>0.996353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERMEDIATE</td>\n",
       "      <td>10</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.114263</td>\n",
       "      <td>4.116835</td>\n",
       "      <td>0.112603</td>\n",
       "      <td>0.996622</td>\n",
       "      <td>0.996281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>10</td>\n",
       "      <td>203</td>\n",
       "      <td>nn.Hardswish</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.135646</td>\n",
       "      <td>2.704047</td>\n",
       "      <td>0.129523</td>\n",
       "      <td>0.996108</td>\n",
       "      <td>0.995575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Trial-ID        Status  Iteration  H_l1      activate        lr  \\\n",
       "0          1  INTERMEDIATE          1   203  nn.Hardswish  0.059275   \n",
       "1          1  INTERMEDIATE          2   203  nn.Hardswish  0.059275   \n",
       "2          1  INTERMEDIATE          3   203  nn.Hardswish  0.059275   \n",
       "3          1  INTERMEDIATE          4   203  nn.Hardswish  0.059275   \n",
       "4          1  INTERMEDIATE          5   203  nn.Hardswish  0.059275   \n",
       "5          1  INTERMEDIATE          6   203  nn.Hardswish  0.059275   \n",
       "6          1  INTERMEDIATE          7   203  nn.Hardswish  0.059275   \n",
       "7          1  INTERMEDIATE          8   203  nn.Hardswish  0.059275   \n",
       "8          1  INTERMEDIATE          9   203  nn.Hardswish  0.059275   \n",
       "9          1  INTERMEDIATE         10   203  nn.Hardswish  0.059275   \n",
       "10         1     COMPLETED         10   203  nn.Hardswish  0.059275   \n",
       "\n",
       "    Objective  net_time  train_loss  train_r_2   val_r_2  \n",
       "0    0.135646  2.704047    0.129523   0.996108  0.995575  \n",
       "1    0.122760  2.871803    0.117678   0.996449  0.995984  \n",
       "2    0.119683  3.023990    0.115277   0.996512  0.996075  \n",
       "3    0.117104  3.170988    0.113159   0.996581  0.996165  \n",
       "4    0.115549  3.329441    0.112192   0.996616  0.996222  \n",
       "5    0.113456  3.504017    0.109955   0.996704  0.996313  \n",
       "6    0.112190  3.649810    0.108733   0.996755  0.996368  \n",
       "7    0.111470  3.817906    0.108536   0.996757  0.996387  \n",
       "8    0.112155  3.968340    0.109908   0.996706  0.996353  \n",
       "9    0.114263  4.116835    0.112603   0.996622  0.996281  \n",
       "10   0.135646  2.704047    0.129523   0.996108  0.995575  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help function\n",
    "from transfer_learning import NeuralNet_sherpa_optimize\n",
    "from dataset_loader import data_loader, get_descriptors, one_filter, data_scaler\n",
    "\n",
    "# modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# New\n",
    "from transfer_learning import MyDataset\n",
    "from Statistics_helper import stratified_cluster_sample\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "import time\n",
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "import sherpa\n",
    "from sklearn.metrics import r2_score\n",
    "parameters = [\n",
    "    sherpa.Continuous(name=\"lr\", range=[0.0002, .1] ),\n",
    "    # sherpa.Discrete(name='Epoch', range=[10,100]),\n",
    "    sherpa.Discrete(name=\"H_l1\", range=[10, 400]),\n",
    "    sherpa.Choice(\n",
    "        name=\"activate\",\n",
    "        range=[\"nn.Hardswish\", \"nn.PReLU\", \"nn.ReLU\", \"nn.Sigmoid\", \"nn.LeakyReLU\"],\n",
    "    ),\n",
    "]\n",
    "algorithm = sherpa.algorithms.RandomSearch(max_num_trials=1)\n",
    "study = sherpa.Study(\n",
    "    parameters=parameters,\n",
    "    algorithm=algorithm,\n",
    "    lower_is_better=False,\n",
    "    disable_dashboard=True,\n",
    ")\n",
    "\n",
    "# file name and data path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_path = os.getcwd()\n",
    "file_name = \"data/CrystGrowthDesign_SI.csv\"\n",
    "\n",
    "\"\"\"\n",
    "Data description.\n",
    "\n",
    "    Descriptors:\n",
    "        'void fraction', 'Vol. S.A.', 'Grav. S.A.', 'Pore diameter Limiting', 'Pore diameter Largest'\n",
    "    Source task:\n",
    "        'H2@100 bar/243K (wt%)'\n",
    "    Target tasks:\n",
    "        'H2@100 bar/130K (wt%)' 'CH4@100 bar/298 K (mg/g)' '5 bar Xe mol/kg' '5 bar Kr mol/kg'\n",
    "\"\"\"\n",
    "\n",
    "descriptor_columns = [\n",
    "    \"void fraction\",\n",
    "    \"Vol. S.A.\",\n",
    "    \"Grav. S.A.\",\n",
    "    \"Pore diameter Limiting\",\n",
    "    \"Pore diameter Largest\",\n",
    "]\n",
    "one_filter_columns = [\"H2@100 bar/243K (wt%)\"]\n",
    "another_filter_columns = [\"H2@100 bar/130K (wt%)\"]\n",
    "\n",
    "# load data\n",
    "data = data_loader(base_path, file_name)\n",
    "data = data.reset_index(drop=True)\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "# parameters\n",
    "input_size = 5\n",
    "output_size = 1\n",
    "\n",
    "# file specifics \n",
    "filename = f\"From-{1}_to_{2}.json\"\n",
    "\n",
    "#format data\n",
    "for trial in study:\n",
    "    learning_rate = trial.parameters[\"lr\"]\n",
    "    df, t_1, t_2, y_1, y_2 = stratified_cluster_sample(\n",
    "        1, data, descriptor_columns, one_filter_columns[0], 5, net_out=True\n",
    "    )\n",
    "    df = df[0]\n",
    "    df=df.drop(\"Cluster\",axis=1)\n",
    "    interest = one_filter_columns[0]\n",
    "    #descriptor_columns.append(\"Cluster\")\n",
    "    features = descriptor_columns\n",
    "\n",
    "    df_train, df_val, y_df_train, y_df_val = train_test_split(\n",
    "        df[features], df[interest], test_size=0.1\n",
    "    )\n",
    "    df_train[interest] = np.array(y_df_train)\n",
    "    df_val[interest] = np.array(y_df_val)\n",
    "    first = MyDataset(df_train, interest, features)\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    second = MyDataset(df_val, interest, features)\n",
    "    val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "    train_loss = []\n",
    "    train_r_2 = []\n",
    "    val_loss = []\n",
    "    val_r_2 = []\n",
    "    net_time = []\n",
    "    #create model\n",
    "    model = NeuralNet_sherpa_optimize(5, 1, trial.parameters).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def train_step(engine, batch):\n",
    "        x, y = batch\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    trainer = Engine(train_step)\n",
    "\n",
    "\n",
    "    metrics = {\"loss\": Loss(criterion), \"r_2\": R2Score()}\n",
    "\n",
    "    #train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "    # train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "    #validation_evaluator = create_supervised_evaluator(\n",
    "    #    model, metrics=metrics, device=device\n",
    "    #)\n",
    "    # validation_evaluator.logger = setup_logger(\"Val Evaluator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    start = time.time()\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "\n",
    "    descriptor_columns = [\n",
    "        \"void fraction\",\n",
    "        \"Vol. S.A.\",\n",
    "        \"Grav. S.A.\",\n",
    "        \"Pore diameter Limiting\",\n",
    "        \"Pore diameter Largest\",\n",
    "    ]\n",
    "    model.fc1.weight.requires_grad = False\n",
    "    model.fc1.bias.requires_grad = False\n",
    "    model.fc2.weight.requires_grad = False\n",
    "    model.fc2.bias.requires_grad = False\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "    )\n",
    "    df, t_1, t_2, y_1, y_2 = stratified_cluster_sample(\n",
    "        1, data, descriptor_columns, another_filter_columns[0], 5, net_out=True\n",
    "    )\n",
    "    df = df[0]\n",
    "    df=df.drop(\"Cluster\",axis=1)\n",
    "    interest = another_filter_columns[0]\n",
    "    #descriptor_columns.append(\"Cluster\")\n",
    "    features = descriptor_columns\n",
    "\n",
    "    df_train, df_val, y_df_train, y_df_val = train_test_split(\n",
    "        df[features], df[interest], test_size=0.1\n",
    "    )\n",
    "    df_train[interest] = np.array(y_df_train)\n",
    "    df_val[interest] = np.array(y_df_val)\n",
    "    first = MyDataset(df_train, interest, features)\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    second = MyDataset(df_val, interest, features)\n",
    "    val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "    interest=another_filter_columns[0]\n",
    "    first = MyDataset(df_train, interest, features)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "    second = MyDataset(df_val, interest, features)\n",
    "    val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "\n",
    "    def train_step_1(engine, batch):\n",
    "        x, y = batch\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    transfer_trainer = Engine(train_step_1)\n",
    "    metrics = {\"loss\": Loss(criterion), \"r_2\": R2Score()}\n",
    "    @transfer_trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
    "    def store_metrics(engine):\n",
    "        end = time.time()\n",
    "        e = engine.state.epoch\n",
    "        out=float(criterion(model(train_loader.dataset.x_train),train_loader.dataset.y_train))\n",
    "        out1=float(r2_score(model(train_loader.dataset.x_train).detach().numpy(),train_loader.dataset.y_train.detach().numpy()))\n",
    "        out2=float(criterion(model(val_loader.dataset.x_train),val_loader.dataset.y_train))\n",
    "        out3=float(r2_score(model(val_loader.dataset.x_train).detach().numpy(),val_loader.dataset.y_train.detach().numpy()))\n",
    "        train_loss.append(out)\n",
    "        train_r_2.append(out1)\n",
    "        val_loss.append(out2)\n",
    "        val_r_2.append(out3)\n",
    "        net_time.append(end-start)\n",
    "        study_dic = {\n",
    "        \"train_loss\": out,\n",
    "        \"train_r_2\": out1,\n",
    "        #\"val_loss\": out2,\n",
    "        \"val_r_2\": out3,\n",
    "        \"net_time\" : end-start,\n",
    "        }\n",
    "        study.add_observation(\n",
    "            trial=trial,iteration=e, objective=out2, context=study_dic\n",
    "        )\n",
    "        if e == transfer_trainer.state.max_epochs:\n",
    "            study.finalize(trial)\n",
    "    transfer_trainer.logger.disabled=True\n",
    "    transfer_trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "study.results.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08e68b6ed4c3e6ab7f2854c6050494f7e80cd03b8cabcf5e57c6479ed706d2af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Transfer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
