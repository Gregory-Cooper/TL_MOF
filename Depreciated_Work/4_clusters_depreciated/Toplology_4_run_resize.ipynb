{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import get_processed_data\n",
    "from sklearn.decomposition import PCA\n",
    "from Functions import make_pca_agg_fit,add_pca_and_graph, prep_data_splits,plot_outline,plot_dendrogram,size_clusters,anaylsis\n",
    "import torch \n",
    "import os\n",
    "from transfer_learning import MyDataset,NeuralNet_sherpa_optimize\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine,Events\n",
    "import time \n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b24c8e4d5c2d>:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  abridge[\"t_cluster\"]=abridge[\"topology\"].map(dic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#if you want certain output and in order of array\n",
    "    # controls graph pca and make pca\n",
    "    # graph of pca cluster annotated\n",
    "    # is graph of median topologies and dendrogram cluster graph\n",
    "    # is the epoch count \n",
    "    # is graphs of first learning\n",
    "    # transfer learning epochs\n",
    "t_name=\"T6\"\n",
    "loud=[0,0,0,0,0,0,0,1]\n",
    "#make cluster run through\n",
    "processed=get_processed_data()\n",
    "#set variablility and number of clusters\n",
    "var=.9\n",
    "g_comp=4\n",
    "#predicting (only supports 1 prediction as of now, but should work with more not tested)\n",
    "interest = [\"H2@100 bar/243K (wt%)\"]\n",
    "interest2=[\"H2@100 bar/243K (wt%)\"]\n",
    "#make PCA to run )\n",
    "#Make PCA on data and generate \n",
    "pc1,pc2,color=make_pca_agg_fit(1,processed,var,g_comp,array_out=True,loud=loud[0])\n",
    "#get unprocessed data\n",
    "data=get_processed_data(unprocessed=True)\n",
    "pca_df=add_pca_and_graph(processed,pc1,pc2,color,graph=loud[1])\n",
    "#puts in old data needed, but can't be processed in PCA\n",
    "pca_df[['MOF ID',interest[0],'topology']]=data[['MOF ID',interest[0],'topology']]\n",
    "\n",
    "#removes some unneeded columns\n",
    "abridge=pca_df[['MOF ID', 'void fraction', 'Vol. S.A.', 'Grav. S.A.','Pore diameter Limiting', 'Pore diameter Largest',interest[0],'topology', 'Pc1', 'Pc2', 'Cluster']]\n",
    "# generate holder for dataframes for looping \n",
    "if loud[2]:\n",
    "    a=abridge.groupby(\"topology\").median()[[\"Pc1\",\"Pc2\"]]\n",
    "    plt.scatter(a[\"Pc1\"],a[\"Pc2\"])\n",
    "    color = AgglomerativeClustering(n_clusters=g_comp).fit_predict(a)\n",
    "    #color=gm.predict(a)\n",
    "    plt.scatter(a[\"Pc1\"],a[\"Pc2\"],c=color)\n",
    "    plt.legend()\n",
    "    plot_outline(abridge)\n",
    "\n",
    "\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "    model = model.fit(a)\n",
    "        \n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(20)\n",
    "    f.set_figheight(20)\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    # plot the top three levels of the dendrogram\n",
    "    plot_dendrogram(model,a, truncate_mode=\"level\", p=3)\n",
    "a=abridge.groupby(\"topology\").median()[[\"Pc1\",\"Pc2\"]]\n",
    "model = AgglomerativeClustering(n_clusters=g_comp)\n",
    "model = model.fit(a)\n",
    "model.labels_\n",
    "dic={}\n",
    "for a,b in zip(a.index,model.labels_):\n",
    "    dic[a]=b\n",
    "abridge[\"t_cluster\"]=abridge[\"topology\"].map(dic)\n",
    "O_M_Cluster=[]\n",
    "for i in sorted(abridge[\"t_cluster\"].unique()):\n",
    "    Temp=abridge[abridge[\"t_cluster\"]==i]\n",
    "    O_M_Cluster.append(Temp)\n",
    "\n",
    "# parameters for neural net\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "input_size = 5\n",
    "output_size = 1\n",
    "resolution=100\n",
    "# specifc for neural net class\n",
    "trial_parameters={\n",
    "    \"lr\" : 0.005,\n",
    "    \"H_l1\" : 250,\n",
    "    \"activate\" :  \"nn.PReLU\"\n",
    "}\n",
    "resize=True\n",
    "#specify cluster\n",
    "meta_trials=100\n",
    "meta=[]\n",
    "if loud[7]:\n",
    "    counter=0\n",
    "for seed,i in enumerate(range(meta_trials)):\n",
    "    np.random.seed(seed)\n",
    "    #create holder for meta anaylsis\n",
    "    mega={}\n",
    "    if resize:\n",
    "        M_Cluster=size_clusters(O_M_Cluster)\n",
    "    else:\n",
    "        M_Cluster=O_M_Cluster\n",
    "    for i in range(len(M_Cluster)):\n",
    "        mega[i]=[]\n",
    "    for count,g in enumerate(M_Cluster):\n",
    "        cluster=count\n",
    "        data = g\n",
    "        #checks for index problem\n",
    "        data = data.reset_index(drop=True)\n",
    "        #computer requirements \n",
    "        device = torch.device(\"cpu\")\n",
    "        base_path = os.getcwd()\n",
    "\n",
    "        features = [\n",
    "            \"void fraction\",\n",
    "            \"Vol. S.A.\",\n",
    "            \"Grav. S.A.\",\n",
    "            \"Pore diameter Limiting\",\n",
    "            \"Pore diameter Largest\",\n",
    "        ]\n",
    "        #create splits \n",
    "        df_train,df_val,df_test=prep_data_splits(data,features,interest)\n",
    "        #create dataset loader objects for looping through data,default test size is .2\n",
    "        first = MyDataset(df_train, interest, features)\n",
    "        train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "\n",
    "        second = MyDataset(df_val, interest, features)\n",
    "        val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "        third = MyDataset(df_test, interest, features)\n",
    "        test_loader=torch.utils.data.DataLoader(third, batch_size=len(df_test))\n",
    "        #intialize holders for data\n",
    "        train_loss = []\n",
    "        train_r_2 = []\n",
    "        val_loss = []\n",
    "        val_r_2 = []\n",
    "        test_loss = []\n",
    "        test_r_2 = []\n",
    "        net_time = []\n",
    "\n",
    "        # initalize model\n",
    "        model = NeuralNet_sherpa_optimize(5, 1, trial_parameters).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        learning_rate = trial_parameters[\"lr\"]\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        #Create engine for running NN, pytorch\n",
    "\n",
    "        def train_step(engine, batch):\n",
    "            x, y = batch\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            return loss.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "        #sets up loss and R scores and stores values\n",
    "        #Note: note sure if can condense this into a module b/c of decorator and namespace\n",
    "        @trainer.on(Events.EPOCH_COMPLETED(every=50))\n",
    "        def store_metrics(engine):\n",
    "            end = time.time()\n",
    "            e = engine.state.epoch\n",
    "            out=float(criterion(model(train_loader.dataset.x_train),train_loader.dataset.y_train))\n",
    "            out1=float(r2_score(model(train_loader.dataset.x_train).detach().numpy(),train_loader.dataset.y_train.detach().numpy()))\n",
    "            out2=float(criterion(model(val_loader.dataset.x_train),val_loader.dataset.y_train))\n",
    "            out3=float(r2_score(model(val_loader.dataset.x_train).detach().numpy(),val_loader.dataset.y_train.detach().numpy()))\n",
    "            out4=float(criterion(model(test_loader.dataset.x_train),test_loader.dataset.y_train))\n",
    "            out5=float(r2_score(model(test_loader.dataset.x_train).detach().numpy(),test_loader.dataset.y_train.detach().numpy()))\n",
    "            train_loss.append(out)\n",
    "            train_r_2.append(out1)\n",
    "            val_loss.append(out2)\n",
    "            val_r_2.append(out3)\n",
    "            test_loss.append(out4)\n",
    "            test_r_2.append(out5)\n",
    "            net_time.append(end-start)\n",
    "            if loud[3]:\n",
    "                print(e)\n",
    "        #Creates timer and runs trainer \n",
    "        start = time.time()\n",
    "        trainer.logger.disabled=True\n",
    "        trainer.run(train_loader, max_epochs=epochs)\n",
    "        plt.show()\n",
    "\n",
    "        if loud[4]:\n",
    "            plt.plot(val_r_2)\n",
    "            plt.plot(train_r_2,label=\"t\")\n",
    "            plt.plot(test_r_2,label=\"real\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.plot(val_loss)\n",
    "            plt.plot(train_loss,label=\"t\")\n",
    "            plt.plot(test_loss,label=\"real\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        #save base model for transfer into other clusters in loop\n",
    "        torch.save(model, f\"{t_name}.ckpt\")\n",
    "        #transfer learning part\n",
    "        #stores meta data\n",
    "        test_loss_holder=[]\n",
    "        test_r2_holder=[]\n",
    "        #loops through clusters\n",
    "        for count,i in enumerate(M_Cluster):\n",
    "            train_loss = []\n",
    "            train_r_2 = []\n",
    "            val_loss = []\n",
    "            val_r_2 = []\n",
    "            test_loss = []\n",
    "            test_r_2 = []\n",
    "            net_time = []\n",
    "            #loading model again\n",
    "            model=torch.load(f\"{t_name}.ckpt\")\n",
    "            descriptor_columns = [\n",
    "                \"void fraction\",\n",
    "                \"Vol. S.A.\",\n",
    "                \"Grav. S.A.\",\n",
    "                \"Pore diameter Limiting\",\n",
    "                \"Pore diameter Largest\",\n",
    "            ]\n",
    "        #turns off model learning\n",
    "            model.fc1.weight.requires_grad = False\n",
    "            model.fc1.bias.requires_grad = False\n",
    "            model.fc2.weight.requires_grad = False\n",
    "            model.fc2.bias.requires_grad = False\n",
    "            interest=interest2\n",
    "            optimizer = optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate\n",
    "            )\n",
    "            #Base Cluster for transfer learning \n",
    "            data = i\n",
    "            #checks for index problem\n",
    "            data = data.reset_index(drop=True)\n",
    "            df_train,df_val,df_test=prep_data_splits(data,features,interest)\n",
    "            #create dataset loader objects for looping through data,default test size is .2\n",
    "            first = MyDataset(df_train, interest, features)\n",
    "            train_loader = torch.utils.data.DataLoader(first, batch_size=batch_size)\n",
    "\n",
    "            second = MyDataset(df_val, interest, features)\n",
    "            val_loader = torch.utils.data.DataLoader(second, batch_size=len(df_val))\n",
    "\n",
    "            third = MyDataset(df_test, interest, features)\n",
    "            test_loader=torch.utils.data.DataLoader(third, batch_size=len(df_test))\n",
    "\n",
    "            #once again making training and testing engine\n",
    "            def train_step_1(engine, batch):\n",
    "                x, y = batch\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                return loss.item()\n",
    "\n",
    "\n",
    "            transfer_trainer = Engine(train_step_1)\n",
    "            n=int(epochs/resolution)\n",
    "            @transfer_trainer.on(Events.EPOCH_COMPLETED(every=n))\n",
    "            def store_metrics(engine):\n",
    "                end = time.time()\n",
    "                e = engine.state.epoch\n",
    "                out=float(criterion(model(train_loader.dataset.x_train),train_loader.dataset.y_train))\n",
    "                out1=float(r2_score(model(train_loader.dataset.x_train).detach().numpy(),train_loader.dataset.y_train.detach().numpy()))\n",
    "                out2=float(criterion(model(val_loader.dataset.x_train),val_loader.dataset.y_train))\n",
    "                out3=float(r2_score(model(val_loader.dataset.x_train).detach().numpy(),val_loader.dataset.y_train.detach().numpy()))\n",
    "                out4=float(criterion(model(test_loader.dataset.x_train),test_loader.dataset.y_train))\n",
    "                out5=float(r2_score(model(test_loader.dataset.x_train).detach().numpy(),test_loader.dataset.y_train.detach().numpy()))\n",
    "                train_loss.append(out)\n",
    "                train_r_2.append(out1)\n",
    "                val_loss.append(out2)\n",
    "                val_r_2.append(out3)\n",
    "                test_loss.append(out4)\n",
    "                test_r_2.append(out5)\n",
    "                net_time.append(end-start)\n",
    "                if loud[5]:\n",
    "                    print(e)\n",
    "            #runs model\n",
    "            transfer_trainer.logger.disabled=True\n",
    "            transfer_trainer.run(train_loader, max_epochs=epochs)\n",
    "            test_loss_holder.append(test_loss)\n",
    "            test_r2_holder.append(test_r_2)\n",
    "        #because error tends to be low, only anaylizing r2 scores\n",
    "            mega[count].append(test_r_2)\n",
    "        #plots all clusers learning in terms of test set\n",
    "        if loud[6]:\n",
    "            plt.title(f\"base {cluster}\")\n",
    "            for count,i in enumerate(test_loss_holder):\n",
    "                plt.plot(i,label=f\"Cluser {count}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.title(f\"base {cluster}\")\n",
    "            for count,i in enumerate(test_r2_holder):\n",
    "                plt.plot(i,label=f\"Cluser {count}\")\n",
    "            plt.legend()\n",
    "    if loud[7]:\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "    meta.append(mega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561\n",
      "['tpt' 'sodb' 'tbo' 'csq' 'stp' 'the' 'srsb' 'bor' 'spn' 'ctn' 'ssa'\n",
      " 'rhrb' 'crs'] Cluster 0\n",
      "561\n",
      "['ftw' 'rht' 'flu' 'reo' 'lvtb' 'gar' 'ocu' 'pcu' 'acs' 'scu' 'nia'] Cluster 1\n",
      "561\n",
      "['nbob' 'lcsb' 'iac' 'ssb' 'pyr' 'soc' 'pto' 'pth' 'diab' 'pts' 'qtz'\n",
      " 'she'] Cluster 2\n",
      "561\n",
      "['bcu' 'bct' 'bcs' 'ith' 'fcu'] Cluster 3\n"
     ]
    }
   ],
   "source": [
    "for i in M_Cluster:\n",
    "    print(len(i))\n",
    "    number=i[\"t_cluster\"].unique()\n",
    "    print(i[\"topology\"].unique(),f\"Cluster {number[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable epoch_conversions is [561, 561, 561, 561]\n",
      "Variable byte is 268\n",
      "Variable epochs is 500\n",
      "Variable resolution is 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "epoch_conversions=[]\n",
    "byte=df_train.iloc[0].memory_usage()\n",
    "for i in M_Cluster:\n",
    "    epoch_conversions.append(len(i))\n",
    "outs=[epoch_conversions,byte,epochs,resolution]\n",
    "o_name=[\"epoch_conversions\",\"byte\",\"epochs\",\"resolution\"]\n",
    "for a,b in zip(outs,o_name):\n",
    "    print(f\"Variable {b} is {a}\")\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce0738356d0cddf65f63e651cde4dcb8456c8dcfca6e76913a71c9f74b06aa6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
